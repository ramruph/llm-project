{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a60395b",
   "metadata": {},
   "source": [
    "# Gradio UI for LLM \n",
    "\n",
    "This project will build a User Interface for a llm project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "690eb571",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "from typing import List\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display, update_display \n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "341f0ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/llms/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# importing gradio \n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cf86aff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing constants\n",
    "ollama_url = 'http://localhost:11434/v1'\n",
    "llama_api_url = \"http://localhost:11434/api/chat\"\n",
    "Headers = {\"Content-Type\": \"application/json\"}\n",
    "Model = \"llama3.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad1911b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A generic system message\n",
    "system_message = \"You are a helpful assistant\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d12663c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the ollama local model\n",
    "\n",
    "def message_llama(prompt):\n",
    "    ollama = OpenAI(base_url=ollama_url, api_key=\"ollama\")\n",
    "\n",
    "    message = [\n",
    "        {\"role\":\"system\", \"content\":system_message},\n",
    "        {\"role\":\"user\", \"content\": prompt}\n",
    "    ]\n",
    "    completion = ollama.chat.completions.create(\n",
    "        model=Model,\n",
    "        messages = message\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "06c6d816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Today's Date: \\nUnfortunately, I'm a large language model, I don't have real-time access to the current date. However, you can share your location or timezone with me, and I'll do my best to provide you with the current date.\\n\\nAlternatively, if you need help with a specific task or calculation that involves dates, feel free to share more information, and I'll be happy to assist you!\""
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_llama(\"What is today's date?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2b3e1f",
   "metadata": {},
   "source": [
    "# Create a UI using Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "332784e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a test function to shout (uppercase)\n",
    "def shout(text):\n",
    "    return text.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02b55f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fn=function, input, output\n",
    "gr.Interface(fn=shout, inputs=\"textbox\", outputs=\"textbox\", allow_flagging=\"never\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aae8fa20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7865\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7865/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A tailored system message that returns in Markdown\n",
    "system_message = \"You are a helpful assistant that responds in markdown\"\n",
    "\n",
    "# creating a Gradio UI for llama llm\n",
    "gr.Interface(fn=message_llama,\n",
    "            inputs=[gr.Textbox(label=\"Your Message:\", lines=6)],\n",
    "            outputs=[gr.Textbox(label=\"Chat Output:\", lines=10)],\n",
    "            flagging_mode=\"never\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8a874e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a stream function for llm response \n",
    "# Yeild -> Generators\n",
    "\n",
    "def stream_llama(prompt):\n",
    "    ollama = OpenAI(base_url=ollama_url, api_key=\"ollama\")\n",
    "\n",
    "    message = [\n",
    "        {\"role\":\"system\", \"content\":system_message},\n",
    "        {\"role\":\"user\", \"content\": prompt}\n",
    "    ]\n",
    "    stream = ollama.chat.completions.create(\n",
    "        model=Model,\n",
    "        messages = message,\n",
    "        stream=True\n",
    "    )\n",
    "    result = \"\"\n",
    "    for chunk in stream:\n",
    "        result += chunk.choices[0].delta.content or \"\"\n",
    "    yield result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a40442",
   "metadata": {},
   "source": [
    "## Generators (the \"yield\" keyword) \n",
    "Why Do We Need Generators?\n",
    "- Memory Efficient : Handle large or infinite data without loading everything into memory.\n",
    "- No List Overhead : Yield items one by one, avoiding full list creation.\n",
    "- Lazy Evaluation : Compute values only when needed, improving performance.\n",
    "- Support Infinite Sequences : Ideal for generating unbounded data like Fibonacci series.\n",
    "- Pipeline Processing : Chain generators to process data in stages efficiently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "390b98da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7869\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7869/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A tailored system message that returns in Markdown\n",
    "system_message = \"You are a helpful assistant that responds in markdown format\"\n",
    "\n",
    "# creating a Gradio UI for llama llm\n",
    "gr.Interface(fn=stream_llama,\n",
    "            inputs=[gr.Textbox(label=\"Your Message:\", lines=6)],\n",
    "            outputs=[gr.Markdown(label=\"Chat Output:\")],\n",
    "            flagging_mode=\"never\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105cec4c",
   "metadata": {},
   "source": [
    "# Adding Deepseek-r1 8b \n",
    "\n",
    "Notes:\n",
    "- Ollama runs its API server on http://localhost:11434/v1 by default.\n",
    "- You don’t need to change that whether you’re calling deepseek-r1:8b, llama3.1, or any other model you’ve pulled.\n",
    "-The only thing that changes is the model name you pass in your request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0700bc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a stream function for DeepSeekllm response \n",
    "\n",
    "def stream_deepseek(prompt):\n",
    "    ollama = OpenAI(base_url=ollama_url, api_key=\"ollama\")\n",
    "\n",
    "    message = [\n",
    "        {\"role\":\"system\", \"content\":system_message},\n",
    "        {\"role\":\"user\", \"content\": prompt}\n",
    "    ]\n",
    "    stream = ollama.chat.completions.create(\n",
    "        model=\"deepseek-r1:8b\",\n",
    "        messages = message,\n",
    "        stream=True\n",
    "    )\n",
    "    result = \"\"\n",
    "    for chunk in stream:\n",
    "        result += chunk.choices[0].delta.content or \"\"\n",
    "    yield result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c0e1d92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the ollama local model\n",
    "\n",
    "def message_deepseek(prompt):\n",
    "    ollama = OpenAI(base_url=ollama_url, api_key=\"ollama\")\n",
    "\n",
    "    message = [\n",
    "        {\"role\":\"system\", \"content\":system_message},\n",
    "        {\"role\":\"user\", \"content\": prompt}\n",
    "    ]\n",
    "    completion = ollama.chat.completions.create(\n",
    "        model=\"deepseek-r1:8b\",\n",
    "        messages = message\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "04bc4f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model functions for Gradio\n",
    "def stream_model(prompt, model):\n",
    "    if model==\"Llama\":\n",
    "        result = stream_llama(prompt)\n",
    "    elif model==\"Deepseek\":\n",
    "        result = stream_deepseek(prompt)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown model\")\n",
    "    yield from result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c3fd0e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7876\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7876/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.Interface(\n",
    "    fn=stream_model, \n",
    "    inputs=[gr.Textbox(label=\"Your Message:\"), gr.Dropdown([\"Llama\",\"Deepseek\"], label=\"Select Model\", value=\"Llama\")],\n",
    "    outputs=[gr.Markdown(label=\"Response:\")],\n",
    "    flagging_mode='never').launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca1737a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
